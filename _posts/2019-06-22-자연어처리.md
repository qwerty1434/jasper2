---
layout: post
current: post
cover:  assets/images/apng.png
navigation: True
title: 자연어처리
date: 2019-06-22 13:25:00
tags: [Getting started]
class: post-template
subclass: 'post tag-getting-started'
author: qwerty1434
---
목표: 하나의 문장을 몇개로 나눠야 할 때 잘 나누는 방법

참고 사이트들]
https://wikidocs.net/22530 - 기본내용,따라해보며 배우기 딱좋음
https://medium.com/@rinu.gour123/nlp-tutorial-ai-with-python-natural-language-processing-ed81fdb3f0a3 - 자연어처리의 분야, 커다란 카테고리의 이름들, 뭘로검색할지 막막할 때 도움이 됐음
https://ratsgo.github.io/natural%20language%20processing/2017/08/16/deepNLP - 기본에서 최신트랜드까지, 개체명 인식,의미역 결정 이라는 용어를 알아낼 수 있었다
  개체명 인식 - 고유단어를 잘 인시하게 하는 기술
  의미역 결정 - 술어와 논항의 구조를 알아내는 기술
https://github.com/kakao/khaiii - 카카오에서 낸거라서  
https://korquad.github.io/ - 데이터셋, 자체리더보드도 있음
https://brunch.co.kr/@mapthecity/25 - 작업에 대한 아이디어를 얻은 곳, Dependency를 기준으로 나누는 문장을 나눈다면 괜찮지 않을까라는 생각을 했음
(https://cloud.google.com/natural-language/?hl=ko - 유료사이트, 기술공개는 안돼있는걸로 봤음)

방법1]
조사를 기준으로 -> 구문처리.ipynb

발전가능성]chunking이라는 키워드를 받아서 청크를 해보면 성능이 어떻게 될지 궁금함

방법2]
https://datascienceschool.net/view-notebook/a0c848e1e2d343d685e6077c35c4203b/
bigram을 활용하는 방향
bigram을 통해 A다음 B단어가 올 조건부 확률을 구한다 -> 문장에서 조건부 확률이 낮은 곳은 서로 같이 쓰지 않는 단어들 -> 문장을 나눠도 어색하지 않을 가능성이 큰 부분
가정1]많이 쓰이는 단어들을 자르기 어색하다
아쉬운점]학습의 개념이라기 보다는 dictionary에 가깝다는 느낌을 받았음(dataset에서 해당 묶음이 몇개있냐를 확인하는 정도?)
"""코드
from nltk import bigrams, word_tokenize
from nltk.util import ngrams

sentence = "I am a boy."
tokens = word_tokenize(sentence)

bigram = bigrams(tokens)
trigram = ngrams(tokens, 3)

for t in bigram:
    print(t)

for t in trigram:
    print(t)


bigram = ngrams(tokens, 2, pad_left=True, pad_right=True, left_pad_symbol="SS", right_pad_symbol="SE")
for t in bigram:
    print(t)
    
def tokenize_and_make_prediction_model(text):
    global sentence
    global freq_words
    global cfreq_brown_2gram
    global cprob_brown_2gram
    sentence = word_tokenize(text)
    freq_words = nltk.FreqDist(sentence)
    cfreq_brown_2gram = nltk.ConditionalFreqDist(nltk.bigrams(sentence))
    cprob_brown_2gram = nltk.ConditionalProbDist(cfreq_brown_2gram, nltk.MLEProbDist)

tokenize_and_make_prediction_model(text)

def test_model(test_sentence):
    test_tokens= word_tokenize(test_sentence)
    test_bigram= bigrams(test_tokens)
    prob_list = []
    for i in test_bigram:
        prob_list.append(cprob_brown_2gram[i[0]].prob(i[1]))
    print(test_tokens)
    print(prob_list)
    return test_tokens,prob_list    
"""
둘중 뭐썼지? 둘다썼나?

"""코드
import nltk
from nltk.corpus import brown

#json데이터 불러오기 - 한글 데이터
import json
json_data = open(file).read()#데이터 출처:https://korquad.github.io/
data = json.loads(json_data)
text = ""#하나로 합치기
for i in range(len(data['data'])):
    for j in range(len(data['data'][i]['paragraphs'])):
        text = text + data['data'][i]['paragraphs'][j]['context']
    

sentence = word_tokenize(text)
#words = brown.words()
words = sentence
#단어의 빈도 측정
freq_brown = nltk.FreqDist(words)#단어의 빈도 측정
len(words)#brown.words()는 100만개의 tokenized word다
list(freq_brown.keys())[:20]
freq_brown.most_common(20)


#ConditionalFreqDist는 해당 단어와 같이 많이 나온 단어를 알려준다
cfreq_brown_2gram = nltk.ConditionalFreqDist(nltk.bigrams(words)) 

cprob_brown_2gram = nltk.ConditionalProbDist(cfreq_brown_2gram, nltk.MLEProbDist)#MLE가 뭐냐


from nltk import bigrams, word_tokenize

#Test워드
sentence ="브이씨앤씨가 7월부터 사고가 발생하면 운전자가 부담하는 차량 손해 면책금을 없애는 정책을 실시한다고 밝혔다."
tokens = word_tokenize(sentence)
bigram = bigrams(tokens)
#bigram = ngrams(tokens, 2, pad_left=True, pad_right=True, left_pad_symbol="SS", right_pad_symbol="SE")#시작과 끝에 토큰 만들기
prob_list = []
for i in bigram:
    print(i)
    print(cprob_brown_2gram[i[0]].prob(i[1]))
    prob_list.append(cprob_brown_2gram[i[0]].prob(i[1]))

print(tokens)
print(prob_list)
"""


데이터 수집]
데이터 수집 다운로드가 되는 데이터들은 다운로드하고 거기에 크롤링으로 수집할 데이터를 합칠 생각이였음
구조는 다 짰고 모을 수 있는데 작업을 시작하지는 않았음 
  ->많은 데이터를 빠르게 모을 수 없다는 단점, 어느사이트에를 긁어야 많이 데이터를 모을 수 있는지 를 해결하지는 못함
"""코드
def get_data(answer):
    for page_num in range(100):
        url = "https://www.venturesquare.net/"+str(page_num)
        try:
            response = urllib.request.urlopen(url)
            new_soup = BeautifulSoup(response.read(),"lxml")
            body = new_soup.find("div",{"class":"post-content"})
            sharedaddy = new_soup.findAll("div",{"class":"sharedaddy"})
            for i in range(len(sharedaddy)):
                body = str(body).replace(str(sharedaddy[i]),"")
            article_body = re.sub("<[^>]*>",'',str(body))    
            answer += article_body
        except:
            continue;
    return answer
an = get_data("")


def load_json_data(file="/Users/vp/Downloads/KorQuAD_v1.0_train.json"):
    import nltk
    from nltk.corpus import brown
    
    #json데이터 불러오기 - 한글 데이터
    import json
    json_data = open(file).read()#데이터 출처:https://korquad.github.io/
    data = json.loads(json_data)
    text = ""#하나로 합치기
    for i in range(len(data['data'])):
        for j in range(len(data['data'][i]['paragraphs'])):
            text = text + data['data'][i]['paragraphs'][j]['context']
    return text

text = load_json_data()

text = text+an
"""


느낀점]
후반부에 회의적은 생각이 많이들었음
대체 얼마나 많은 데이터를 수집해야 기본적인 성능이라도 낼 건가
빈도의 유무가 꼭 문장의 구분이 될 수 없다는 생각을 했다
ex)""는 많이 나오는 표현이지만 둘 사이를 띄어쓰는게 맞아보인다 
문맥상 말이되지만 희소한 단어라면? "이 그림의 골계미가 나를 끌어당겼다" ->'골계미'는 '그림의','나를' 둘다 점수가 0일 확률이 크다
7월부터 -> 이게 다른 문장에서 또한번 나올려면 얼마나 많은 데이터가 있어야 될까? , 7월부터는 앞이랑 붙나 뒤랑 붙나 상관없을 가능성이 크다?
명사를 나열한 문장은 학습이 안된다 (뉴스기사제목)


발전가능성]
단어를 기본형으로 처리하면 조금 더 빨리 원하는 성능에 도달하지 않을까 생각했음
방법3]
앙상블 - 처음에는 확률로 접근하고 확률이 너무 낮으면 조사를 기준으로 자르기

추가]
기존 작업: combination을 바탕으로 모든 경우의 띄어쓰기 경우의 수를 구한 후 적절한 길이를 구하는 방법
1.콤비네이션으로 모든 띄어쓰기의 경우의 수 구하는 방법
