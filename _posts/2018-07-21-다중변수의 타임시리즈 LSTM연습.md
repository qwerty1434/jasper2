---
layout: post
current: post
cover: 'assets/images/piano.jpg'
navigation: true
title: 다중변수의 타임시리즈 LSTM연습
date: 2018-07-21 17:06:00
tags: fiction
class: post-template
subclass: 'post tag-fiction'
author: qwerty1434
---
Multivariate Time Series Forecasting with LSTMs in Keras
============

*LSTM이란: 


LSTM으로 다변량(?) 시계열 데이터 예측

다양한 시계열 변수(time_series of x1,x2,x3...)를 바탕으로 
시계열 변수(time_seires of y1) 예측하기


실습예제 : 이전 24시간동안의 기상상태와 오염도를 바탕으로 앞으로의 오염도를 예측해 보기



## 모든 코드와 내용 설명은 아래의 출처에서 가져왔습니다.

#### 출처: https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/




### 1.데이터 다운로드
* 다운: https://archive.ics.uci.edu/ml/machine-learning-databases/00381/

* 파일명을 raw.csv로 저장해 줍니다



### 2.데이터 정제
##### 데이터 분석에 앞서 데이터를 사용하기 용이하게 전처리 합니다

* 이번 예제에 사용될 모듈들을 불러옵니다
```c
import pandas as pd
import numpy as np
from pandas import read_csv
from datetime import datetime
from pandas import DataFrame
from sklearn import preprocessing
from sklearn.metrics import mean_squared_error
from matplotlib import pyplot
```


* year,month,day,hour를 datetime으로 변환한 값을 index로 쓰고
```c
def parse(x):
	return datetime.strptime(x, '%Y %m %d %H')
dataset = read_csv('raw.csv',  parse_dates = [['year', 'month', 'day', 'hour']],
                   index_col=0, date_parser=parse)
```

* No변수를 삭제해 주고
```c
dataset.drop('No', axis=1, inplace=True)
```

* 변수와 index이름을 재정의 해주고
```c
dataset.columns = ['pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']
dataset.index.name = 'date'
```

* pollution의 빈값을 0으로 채우고
```c
dataset['pollution'].fillna(0, inplace=True)
```

* 첫 24시간 데이터는 제외한
```c
dataset = dataset[24:]
```

* 데이터를 pollution.csv로 저장합니다
```c
dataset.to_csv('pollution.csv')
```


### 3.데이터 Scaling
##### 데이터 모양을 변형하고고 변수들을 정규화(normalize) 시킵니다
* pollition.csv를 불러와서
```c
dataset = read_csv('Downloads/pollution.csv', header=0, index_col=0)
values = dataset.values
```
* 문자로 나타나 있는 범주형 데이터인 wind_dir를 숫자로 encoding하고
```c
encoder = preprocessing.LabelEncoder()
values[:,4] = encoder.fit_transform(values[:,4]) #범주형 wind_dir 변수를 숫자로 인코딩 해 준다
values = values.astype('float32')
```


* minmaxscaler로 모든 값을 0~1사이로로 바꿔준 후
```c
scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values) #값을 0~1로 떨어뜨린다
```

* t-1시점데이터와 t시점데이터를 하나의 행으로 가지는 데이터프레임을 reframed변수로 선언하고
```c
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
	n_vars = 1 if type(data) is list else data.shape[1]
	df = DataFrame(data)
	cols, names = list(), list()
	# input sequence (t-n, ... t-1)
	for i in range(n_in, 0, -1):
		cols.append(df.shift(i))
		names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
	# forecast sequence (t, t+1, ... t+n)
	for i in range(0, n_out):
		cols.append(df.shift(-i))
		if i == 0:
			names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
		else:
			names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
	# put it all together
	agg = pd.concat(cols, axis=1)
	agg.columns = names
	# drop rows with NaN values
	if dropnan:
		agg.dropna(inplace=True)
	return agg
```

```c
reframed = series_to_supervised(scaled, 1, 1) #t-1시점,t시점 데이터를 한 행으로 둔다     
```
* 사용하지 않는 변수는 제외해 줍니다
```c
reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)
```
##### series_to_supervised함수의 n_in,n_out에 따라 가져올 시점의 데이터가 달라집니다


##### (만약 n_in = 3, n_out = 3이라면 t-3 , t-2 , t-1 , t , t+1 , t+2 시점의 데이터가 하나의 행이 됩니다)

![series_to_supervised](/assets/images/LSTM/Series_to_supervised.jpeg)
실제 사용된 2차원 데이터도 위 그림과 마찬가지 입니다 조금 더 뚱뚱(?)해진다고 생각하시면 됩니다
(이런 그림은 멋지게 프로그래밍해서 보여줘야 하는데 손그림이라 부끄럽네요)

### 4.데이터 train/test split
##### LSTM을 사용하기 위해 모델학습용 데이터(train)와 정확도를 확인하기 위한 데이터(test)를 구분시켜 줍니다
```c
values = reframed.values
n_train_hours = 365 * 24 #1년치 데이터만 가져온다
train = values[:n_train_hours, :]
test = values[n_train_hours:, :]
# split into input and outputs
train_X, train_y = train[:, :-1], train[:, -1]
test_X, test_y = test[:, :-1], test[:, -1]
# reshape input to be 3D [samples, timesteps, features]
train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))
test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))
```
* (8760,9) 형태의 train데이터가 (8760,1,8)train_X , (8760,)train_Y의 3차원 데이터(train_X만)가 됩니다
* 왜 3차원이 되어야 하는걸까요?? 정확히는 모르지만 개인적으로 LSTM모델이 3차원 데이터를 input으로 요구해서가 아닐까 생각합니다

### 5.모델 빌드
##### LSTM 모델을 구축하고 train데이터를 이용해 학습시켜 줍니다
* "We will define the LSTM with 50 neurons in the first hidden layer and 1 neuron in the output layer for predicting pollution"
```c
model = Sequential()
model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dense(1))
model.compile(loss='mae', optimizer='adam')
```


* 모델을 훈련시킨 후 
```c
history = model.fit(train_X, train_y, epochs=50, batch_size=72, 
          validation_data=(test_X, test_y), verbose=2, shuffle=False)
```


* loss를 그래프로 확인합니다
```c
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()
```

![LSTM_model_train](/assets/images/LSTM/LSTM_train.png)


### 6.예측 및 모델평가
##### 학습이 끝난 모델로 test데이터를 예측하고, 실제 결과와 비교해 봅니다
```c
# make a prediction
yhat = model.predict(test_X)
test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))
# invert scaling for forecast
inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)
inv_yhat = scaler.inverse_transform(inv_yhat)
inv_yhat = inv_yhat[:,0]
# invert scaling for actual
test_y = test_y.reshape((len(test_y), 1))
inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)
inv_y = scaler.inverse_transform(inv_y)
inv_y = inv_y[:,0]
# calculate RMSE
rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
```

### 느낀점: 

* LSTM모델을 활용시 주변시점 데이터를 t시점과 동일 선상에 오게끔 변형한 후 분석하는 과정

* 모델을 만들고 사용할 때는 차원을 잘 맞춰줘야 한다


(차원이 다르면 대략 이런느낌 ↓)


![텍스트](http://mblogthumb3.phinf.naver.net/20160713_190/prtty5826_1468408643474i17GU_GIF/19957ed01a76df17812898074f1c080a.gif?type=w800)
